{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams=dict()\n",
    "# Hyper-parameters\n",
    "hparams['hidden_size']   = 100  # hidden layer's size\n",
    "hparams['seq_length']    = 20   # number of steps to unroll\n",
    "hparams['learning_rate'] = 1e-3\n",
    "hparams['num_epochs'] = 10\n",
    "hparams['dropout'] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vocab\n",
    "data_builder = vocab.DataBuilder()\n",
    "data_vocab = data_builder.build_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = dict()\n",
    "conf['batch_size'] = 256\n",
    "conf['vocab_size'] = len(data_vocab.word2ind)\n",
    "conf['embed_size'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((929589,), (73760,), (82430,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_builder.get_data(word_vocab=data_vocab,dataset='train')\n",
    "valid = data_builder.get_data(word_vocab=data_vocab,dataset='valid')\n",
    "test = data_builder.get_data(word_vocab=data_vocab,dataset='test')\n",
    "train.shape,valid.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholder(hparams,conf):\n",
    "    X = tf.placeholder(shape=(None,hparams['seq_length']),dtype=tf.int32)\n",
    "    Y = tf.placeholder(shape=(None,hparams['seq_length']),dtype=tf.float32)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(hparams,conf):\n",
    "    softmax_w = tf.get_variable(name='softmax_w', shape=[hparams['hidden_size'],conf['vocab_size']],\n",
    "                                dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    softmax_b = tf.get_variable(name='softmax_b', shape=[1,conf['vocab_size']],\n",
    "                                dtype=tf.float32,initializer=tf.initializers.zeros())\n",
    "    parameters = {\"W\":softmax_w,\"b\":softmax_b}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_embeddings(X,hparams,conf):\n",
    "    #(batch,sequence_length,embedding_size)\n",
    "    with tf.device('/cpu:0'):\n",
    "        L = tf.get_variable(\n",
    "            name=\"L\", shape=(conf['vocab_size'], conf['embed_size']),\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        embeddings = tf.nn.embedding_lookup(\n",
    "            params=L, ids=X, name='embeddings')\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters,hparams,conf):\n",
    "    X = tf.unstack(X,hparams['seq_length'],1)\n",
    "    \n",
    "    rnn_cell = rnn.BasicLSTMCell(hparams['hidden_size'])\n",
    "    outputs, states = rnn.static_rnn(cell=rnn_cell,inputs=X,dtype=tf.float32)\n",
    "    output_sequence_logits = []\n",
    "    for output in outputs:\n",
    "        output_sequence_logits.append(tf.matmul(output,parameters[\"W\"]) + parameters[\"b\"])\n",
    "    print(output_sequence_logits[0].shape)\n",
    "    return output_sequence_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(output_sequence_logits, Y, hparams, conf):\n",
    "    w = tf.ones([conf['batch_size'], hparams['seq_length']])\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits=output_sequence_logits,\n",
    "            targets=tf.cast(Y,tf.int32),\n",
    "            weights=w, average_across_batch= True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(data,seq_len):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    num_of_x = data.shape[0]//seq_len\n",
    "    for i in range(num_of_x):\n",
    "        X.append(data[i*seq_len:(i+1)*seq_len])\n",
    "        Y.append(data[i*seq_len + 1:(i+1)*seq_len + 1])\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mini_batches(X,Y,batch_size):\n",
    "    num_of_input = X.shape[0]\n",
    "    num_of_batches = int(num_of_input/batch_size)\n",
    "    mini_batches = []\n",
    "    for i in range(num_of_batches):\n",
    "        start_ind = i*batch_size\n",
    "        batch_x = X[start_ind:start_ind+batch_size]\n",
    "        batch_y = Y[start_ind:start_ind+batch_size]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "    '''if num_of_input%batch_size != 0:\n",
    "        start_ind = num_of_batches*batch_size\n",
    "        batch_x = X[start_ind:]\n",
    "        batch_y = Y[start_ind:]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "    '''\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, hparams, conf):\n",
    "    ops.reset_default_graph()\n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_placeholder(hparams,conf)\n",
    "    parameters = initialize_parameters(hparams,conf)\n",
    "    Z = add_embeddings(X, hparams, conf)\n",
    "    logits = forward_propagation(Z,parameters,hparams, conf)\n",
    "    \n",
    "    output = tf.reshape(tf.concat(values=logits,axis=1), shape=(-1,hparams['seq_length'],conf['vocab_size']))\n",
    "    cost = compute_cost(output,Y,hparams,conf)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams['learning_rate']).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init) #run init\n",
    "        \n",
    "        batch_size = conf['batch_size']\n",
    "        num_minibatches = int(X_train.shape[0]/batch_size)\n",
    "        \n",
    "        for epoch in range(hparams['num_epochs']):\n",
    "            minibatch_cost = 0\n",
    "            minibatches = get_mini_batches(X_train,Y_train,batch_size)\n",
    "            for ind,minibatch in enumerate(minibatches):\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                _ , temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            costs.append(minibatch_cost)\n",
    "            print(\"training cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "            \n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(X_valid.shape[0]/batch_size)\n",
    "            for ind,minibatch in enumerate(get_mini_batches(X_valid,Y_valid,batch_size)):\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                temp_cost = sess.run(cost,feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            print(\"validation cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "        \n",
    "        minibatch_cost = 0\n",
    "        num_minibatches = int(X_test.shape[0]/batch_size)\n",
    "        for ind,minibatch in enumerate(get_mini_batches(X_test,Y_test,batch_size)):\n",
    "            (minibatch_X,minibatch_Y) = minibatch\n",
    "            temp_cost = sess.run(cost,feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "        print(\"Test cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "                \n",
    "    return costs,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train=get_X_Y(train,hparams['seq_length'])\n",
    "X_valid,Y_valid=get_X_Y(valid,hparams['seq_length'])\n",
    "X_test,Y_test=get_X_Y(test,hparams['seq_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10000)\n",
      "training cost after epoch:0 = 7.05222058612\n",
      "validation cost after epoch:0 = 6.61612427235\n",
      "training cost after epoch:1 = 82.5286626617\n",
      "validation cost after epoch:1 = 6.47134852409\n",
      "training cost after epoch:2 = 80.9437227448\n",
      "validation cost after epoch:2 = 6.37231449286\n",
      "training cost after epoch:3 = 79.5073924065\n",
      "validation cost after epoch:3 = 6.25257352988\n",
      "training cost after epoch:4 = 77.5542296966\n",
      "validation cost after epoch:4 = 6.10589118799\n",
      "training cost after epoch:5 = 76.0143843293\n",
      "validation cost after epoch:5 = 6.01487825314\n",
      "training cost after epoch:6 = 74.863173008\n",
      "validation cost after epoch:6 = 5.93931269646\n",
      "training cost after epoch:7 = 73.8668599725\n",
      "validation cost after epoch:7 = 5.87432465951\n",
      "training cost after epoch:8 = 72.9658233523\n",
      "validation cost after epoch:8 = 5.8117120266\n",
      "training cost after epoch:9 = 72.0451683203\n",
      "validation cost after epoch:9 = 5.74589812756\n",
      "Test cost after epoch:9 = 5.74589812756\n"
     ]
    }
   ],
   "source": [
    "_, parameters = model(X_train, Y_train, X_valid, Y_valid,hparams,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.664574275601254"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**5.74589812756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

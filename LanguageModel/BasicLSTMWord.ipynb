{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.framework import ops\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams=dict()\n",
    "# Hyper-parameters\n",
    "hparams['hidden_size']   = 512  # hidden layer's size\n",
    "hparams['seq_length']    = 30   # number of steps to unroll\n",
    "hparams['learning_rate'] = 1e-3\n",
    "hparams['num_epochs'] = 60\n",
    "hparams['dropout'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vocab\n",
    "data_builder = vocab.DataBuilder()\n",
    "data_vocab = data_builder.build_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = dict()\n",
    "conf['batch_size'] = 256\n",
    "conf['vocab_size'] = len(data_vocab.word2ind)\n",
    "conf['embed_size'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((929589,), (73760,), (82430,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_builder.get_data(word_vocab=data_vocab,dataset='train')\n",
    "valid = data_builder.get_data(word_vocab=data_vocab,dataset='valid')\n",
    "test = data_builder.get_data(word_vocab=data_vocab,dataset='test')\n",
    "train.shape,valid.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholder(hparams,conf):\n",
    "    X = tf.placeholder(shape=(None,hparams['seq_length']),dtype=tf.int32)\n",
    "    Y = tf.placeholder(shape=(None,hparams['seq_length']),dtype=tf.float32)\n",
    "    dropout = tf.placeholder(dtype=tf.float32)\n",
    "    return X,Y,dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(hparams,conf):\n",
    "    softmax_w = tf.get_variable(name='softmax_w', shape=[hparams['hidden_size'],conf['vocab_size']],\n",
    "                                dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    softmax_b = tf.get_variable(name='softmax_b', shape=[1,conf['vocab_size']],\n",
    "                                dtype=tf.float32,initializer=tf.initializers.zeros())\n",
    "    parameters = {\"W\":softmax_w,\"b\":softmax_b}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_embeddings(X,hparams,conf):\n",
    "    #(batch,sequence_length,embedding_size)\n",
    "    with tf.device('/cpu:0'):\n",
    "        L = tf.get_variable(\n",
    "            name=\"L\", shape=(conf['vocab_size'], conf['embed_size']),\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        embeddings = tf.nn.embedding_lookup(\n",
    "            params=L, ids=X, name='embeddings')\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters,hparams,conf, keep = 1):\n",
    "    X = tf.unstack(X,hparams['seq_length'],1)\n",
    "    \n",
    "    with tf.variable_scope(\"RNN1\"):\n",
    "        rnn_cell = rnn.BasicLSTMCell(hparams['hidden_size'])\n",
    "        outputs1, states = rnn.static_rnn(cell=rnn_cell,inputs=X,dtype=tf.float32)\n",
    "        for i in range(len(outputs1)):\n",
    "            outputs1[i] = tf.nn.dropout(outputs1[i], keep_prob=keep)\n",
    "            \n",
    "    with tf.variable_scope(\"RNN2\"):\n",
    "        rnn_cell2 = rnn.BasicLSTMCell(hparams['hidden_size'])\n",
    "        outputs2, states = rnn.static_rnn(cell=rnn_cell2, inputs=outputs1, dtype=tf.float32)\n",
    "\n",
    "    output_sequence_logits = []\n",
    "    for output in outputs2:\n",
    "        output_sequence_logits.append(tf.matmul(output,parameters[\"W\"]) + parameters[\"b\"])\n",
    "        \n",
    "    return output_sequence_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(output_sequence_logits, Y, hparams, conf):\n",
    "    w = tf.ones([conf['batch_size'], hparams['seq_length']])\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits=output_sequence_logits,\n",
    "            targets=tf.cast(Y,tf.int32),\n",
    "            weights=w, average_across_batch= True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_Y(data,seq_len):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    num_of_x = int(math.ceil(data.shape[0]/seq_len))\n",
    "    required_zero_pad = seq_len * num_of_x - data.shape[0] + 1\n",
    "    data = np.pad(data,(0,required_zero_pad),mode='constant',constant_values=(0,0))\n",
    "    num_of_x = data.shape[0]//seq_len\n",
    "    for i in range(num_of_x):\n",
    "        X.append(data[i*seq_len:(i+1)*seq_len])\n",
    "        Y.append(data[(i*seq_len + 1):((i+1)*seq_len + 1)])\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mini_batches(X,Y,batch_size):\n",
    "    num_of_input = X.shape[0]\n",
    "    num_of_batches = int(num_of_input/batch_size)\n",
    "    mini_batches = []\n",
    "    for i in range(num_of_batches):\n",
    "        start_ind = i*batch_size\n",
    "        batch_x = X[start_ind:start_ind+batch_size]\n",
    "        batch_y = Y[start_ind:start_ind+batch_size]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "    '''if num_of_input%batch_size != 0:\n",
    "        start_ind = num_of_batches*batch_size\n",
    "        batch_x = X[start_ind:]\n",
    "        batch_y = Y[start_ind:]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "    '''\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, hparams, conf, load=False,epoch_to_load=0):\n",
    "    ops.reset_default_graph()\n",
    "    costs = []\n",
    "    \n",
    "    X, Y, drop_out = create_placeholder(hparams,conf)\n",
    "    \n",
    "    parameters = initialize_parameters(hparams,conf)\n",
    "        \n",
    "    Z = add_embeddings(X, hparams, conf)\n",
    "    logits = forward_propagation(Z,parameters,hparams, conf, keep=drop_out)\n",
    "    \n",
    "    output = tf.reshape(tf.concat(values=logits,axis=1), shape=(-1,hparams['seq_length'],conf['vocab_size']))\n",
    "    cost = compute_cost(output,Y,hparams,conf)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams['learning_rate']).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    batch_size = conf['batch_size']\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init) #run init\n",
    "        \n",
    "        if load:\n",
    "            saver.restore(sess, './LSTM_checkpoints/LSTM_2_epoch_' + str(epoch_to_load) + '.ckpt')\n",
    "        \n",
    "        for epoch in range(hparams['num_epochs']):\n",
    "            minibatch_cost = 0\n",
    "            minibatches = get_mini_batches(X_train,Y_train,batch_size)\n",
    "            num_minibatches = int(X_train.shape[0]/batch_size)\n",
    "            \n",
    "            for ind,minibatch in enumerate(minibatches):\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                _ , temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y, drop_out:hparams['dropout']})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            costs.append(minibatch_cost)\n",
    "            print(\"training cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "            \n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(X_valid.shape[0]/batch_size)\n",
    "            \n",
    "            for ind,minibatch in enumerate(get_mini_batches(X_valid,Y_valid,batch_size)):\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                temp_cost = sess.run(cost,feed_dict={X:minibatch_X,Y:minibatch_Y,drop_out:1})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            print(\"validation cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "            \n",
    "            if epoch % 5 ==0:\n",
    "                saver.save(sess,'./LSTM_checkpoints/LSTM_2_epoch_' + str(epoch) + '.ckpt')\n",
    "            \n",
    "        minibatch_cost = 0\n",
    "        num_minibatches = int(X_test.shape[0]/batch_size)\n",
    "        for ind,minibatch in enumerate(get_mini_batches(X_test,Y_test,batch_size)):\n",
    "            (minibatch_X,minibatch_Y) = minibatch\n",
    "            temp_cost = sess.run(cost,feed_dict={X:minibatch_X,Y:minibatch_Y, drop_out:1})\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "        print(\"Test cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "                \n",
    "    return costs,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,Y_train=get_X_Y(train,hparams['seq_length'])\n",
    "X_valid,Y_valid=get_X_Y(valid,hparams['seq_length'])\n",
    "X_test,Y_test=get_X_Y(test,hparams['seq_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training cost after epoch:0 = 6.96976686903\n",
      "validation cost after epoch:0 = 6.67788161172\n",
      "training cost after epoch:1 = 6.68340827217\n",
      "validation cost after epoch:1 = 6.64372316996\n",
      "training cost after epoch:2 = 6.66476753724\n",
      "validation cost after epoch:2 = 6.6393081877\n",
      "training cost after epoch:3 = 6.66222205832\n",
      "validation cost after epoch:3 = 6.63623581992\n",
      "training cost after epoch:4 = 6.63096858253\n",
      "validation cost after epoch:4 = 6.55859576331\n",
      "training cost after epoch:5 = 6.52912417325\n",
      "validation cost after epoch:5 = 6.47276968426\n",
      "training cost after epoch:6 = 6.45873050059\n",
      "validation cost after epoch:6 = 6.41424809562\n",
      "training cost after epoch:7 = 6.40194779782\n",
      "validation cost after epoch:7 = 6.3720937305\n",
      "training cost after epoch:8 = 6.3531440191\n",
      "validation cost after epoch:8 = 6.3299735917\n",
      "training cost after epoch:9 = 6.31207664742\n",
      "validation cost after epoch:9 = 6.29673888948\n",
      "training cost after epoch:10 = 6.27186824468\n",
      "validation cost after epoch:10 = 6.26197327508\n",
      "training cost after epoch:11 = 6.23536233666\n",
      "validation cost after epoch:11 = 6.2299841245\n",
      "training cost after epoch:12 = 6.19818955808\n",
      "validation cost after epoch:12 = 6.19577307171\n",
      "training cost after epoch:13 = 6.1603284158\n",
      "validation cost after epoch:13 = 6.15854358673\n",
      "training cost after epoch:14 = 6.12126279468\n",
      "validation cost after epoch:14 = 6.12075122197\n",
      "training cost after epoch:15 = 6.08189470118\n",
      "validation cost after epoch:15 = 6.08513736725\n",
      "training cost after epoch:16 = 6.04336771295\n",
      "validation cost after epoch:16 = 6.04230525759\n",
      "training cost after epoch:17 = 5.99652907868\n",
      "validation cost after epoch:17 = 5.9937092993\n",
      "training cost after epoch:18 = 5.93973378505\n",
      "validation cost after epoch:18 = 5.93622732162\n",
      "training cost after epoch:19 = 5.87809874795\n",
      "validation cost after epoch:19 = 5.88287178675\n",
      "training cost after epoch:20 = 5.82002453371\n",
      "validation cost after epoch:20 = 5.83331129286\n",
      "training cost after epoch:21 = 5.76392334946\n",
      "validation cost after epoch:21 = 5.78361961577\n",
      "training cost after epoch:22 = 5.7072623308\n",
      "validation cost after epoch:22 = 5.73573239644\n",
      "training cost after epoch:23 = 5.65354467818\n",
      "validation cost after epoch:23 = 5.69264772203\n",
      "training cost after epoch:24 = 5.60079405919\n",
      "validation cost after epoch:24 = 5.65085607105\n",
      "training cost after epoch:25 = 5.55000487241\n",
      "validation cost after epoch:25 = 5.60765716765\n",
      "training cost after epoch:26 = 5.49843949326\n",
      "validation cost after epoch:26 = 5.56850131353\n",
      "training cost after epoch:27 = 5.45032958354\n",
      "validation cost after epoch:27 = 5.53243663576\n",
      "training cost after epoch:28 = 5.40529247552\n",
      "validation cost after epoch:28 = 5.50118245019\n",
      "training cost after epoch:29 = 5.36243816644\n",
      "validation cost after epoch:29 = 5.4734298918\n",
      "training cost after epoch:30 = 5.32176721983\n",
      "validation cost after epoch:30 = 5.44672436184\n",
      "training cost after epoch:31 = 5.28433701224\n",
      "validation cost after epoch:31 = 5.42382097244\n",
      "training cost after epoch:32 = 5.24827497262\n",
      "validation cost after epoch:32 = 5.40039708879\n",
      "training cost after epoch:33 = 5.21140695209\n",
      "validation cost after epoch:33 = 5.37728447384\n",
      "training cost after epoch:34 = 5.17735114768\n",
      "validation cost after epoch:34 = 5.35794305801\n",
      "training cost after epoch:35 = 5.14433480491\n",
      "validation cost after epoch:35 = 5.33884657754\n",
      "training cost after epoch:36 = 5.11219676861\n",
      "validation cost after epoch:36 = 5.32089318169\n",
      "training cost after epoch:37 = 5.08215168882\n",
      "validation cost after epoch:37 = 5.3058397505\n",
      "training cost after epoch:38 = 5.05204101436\n",
      "validation cost after epoch:38 = 5.29208903843\n",
      "training cost after epoch:39 = 5.02321286635\n",
      "validation cost after epoch:39 = 5.27725580004\n",
      "training cost after epoch:40 = 4.99525129302\n",
      "validation cost after epoch:40 = 5.26480203205\n",
      "training cost after epoch:41 = 4.96684749461\n",
      "validation cost after epoch:41 = 5.25202385585\n",
      "training cost after epoch:42 = 4.93886341142\n",
      "validation cost after epoch:42 = 5.24098030726\n",
      "training cost after epoch:43 = 4.91283304041\n",
      "validation cost after epoch:43 = 5.23015202416\n",
      "training cost after epoch:44 = 4.88648191956\n",
      "validation cost after epoch:44 = 5.22378381093\n",
      "training cost after epoch:45 = 4.86153253445\n",
      "validation cost after epoch:45 = 5.21594609155\n",
      "training cost after epoch:46 = 4.83611647157\n",
      "validation cost after epoch:46 = 5.20605622398\n",
      "training cost after epoch:47 = 4.81109415007\n",
      "validation cost after epoch:47 = 5.19920227263\n",
      "training cost after epoch:48 = 4.7884314277\n",
      "validation cost after epoch:48 = 5.1922516293\n",
      "training cost after epoch:49 = 4.76253027167\n",
      "validation cost after epoch:49 = 5.19010310703\n",
      "training cost after epoch:50 = 4.74001724858\n",
      "validation cost after epoch:50 = 5.18404727512\n",
      "training cost after epoch:51 = 4.71714426466\n",
      "validation cost after epoch:51 = 5.17828003565\n",
      "training cost after epoch:52 = 4.69519876449\n",
      "validation cost after epoch:52 = 5.17245986727\n",
      "training cost after epoch:53 = 4.6731985305\n",
      "validation cost after epoch:53 = 5.1706515948\n",
      "training cost after epoch:54 = 4.65178424662\n",
      "validation cost after epoch:54 = 5.16711356905\n",
      "training cost after epoch:55 = 4.62965296123\n",
      "validation cost after epoch:55 = 5.16667207082\n",
      "training cost after epoch:56 = 4.60677049968\n",
      "validation cost after epoch:56 = 5.16317907969\n",
      "training cost after epoch:57 = 4.58607409611\n",
      "validation cost after epoch:57 = 5.16464678446\n",
      "training cost after epoch:58 = 4.56566696325\n",
      "validation cost after epoch:58 = 5.16433699926\n",
      "training cost after epoch:59 = 4.54521210923\n",
      "validation cost after epoch:59 = 5.1662279235\n",
      "Test cost after epoch:59 = 5.1662279235\n"
     ]
    }
   ],
   "source": [
    "_, parameters = model(X_train, Y_train, X_valid, Y_valid,hparams,conf,load=False,epoch_to_load=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175.25252319498784"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(5.1662279235)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do<br>\n",
    "<ul>\n",
    "<li>Hyperparameter Tuning</li>\n",
    "<li>Add more LSTM layer with dropout in between</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams=dict()\n",
    "# Hyper-parameters\n",
    "hparams['hidden_size']   = 200  # hidden layer's size\n",
    "hparams['seq_length']    = 12   # number of steps to unroll\n",
    "hparams['learning_rate'] = 1e-3\n",
    "hparams['num_epochs'] = 40\n",
    "hparams['dropout'] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vocab\n",
    "data_builder = vocab.DataBuilder()\n",
    "data_vocab = data_builder.build_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = dict()\n",
    "conf['batch_size'] = 256\n",
    "conf['vocab_size'] = len(data_vocab.word2ind)\n",
    "conf['embed_size'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((929589,), (73760,), (82430,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_builder.get_data(word_vocab=data_vocab,dataset='train')\n",
    "valid = data_builder.get_data(word_vocab=data_vocab,dataset='valid')\n",
    "test = data_builder.get_data(word_vocab=data_vocab,dataset='test')\n",
    "train.shape,valid.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholder(hparams,conf):\n",
    "    X = tf.placeholder(shape=(None,hparams['seq_length']),dtype=tf.int32)\n",
    "    Y = tf.placeholder(shape=(None,hparams['seq_length']),dtype=tf.float32)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(hparams,conf):\n",
    "    softmax_w = tf.get_variable(name='softmax_w', shape=[hparams['hidden_size'],conf['vocab_size']],\n",
    "                                dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    softmax_b = tf.get_variable(name='softmax_b', shape=[1,conf['vocab_size']],\n",
    "                                dtype=tf.float32,initializer=tf.initializers.zeros())\n",
    "    parameters = {\"W\":softmax_w,\"b\":softmax_b}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_embeddings(X,hparams,conf):\n",
    "    #(batch,sequence_length,embedding_size)\n",
    "    with tf.device('/cpu:0'):\n",
    "        L = tf.get_variable(\n",
    "            name=\"L\", shape=(conf['vocab_size'], conf['embed_size']),\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        embeddings = tf.nn.embedding_lookup(\n",
    "            params=L, ids=X, name='embeddings')\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters,hparams,conf):\n",
    "    X = tf.unstack(X,hparams['seq_length'],1)\n",
    "    \n",
    "    rnn_cell = rnn.BasicLSTMCell(hparams['hidden_size'])\n",
    "    outputs, states = rnn.static_rnn(cell=rnn_cell,inputs=X,dtype=tf.float32)\n",
    "    output_sequence_logits = []\n",
    "    for output in outputs:\n",
    "        output_sequence_logits.append(tf.matmul(output,parameters[\"W\"]) + parameters[\"b\"])\n",
    "    print(output_sequence_logits[0].shape)\n",
    "    return output_sequence_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(output_sequence_logits, Y, hparams, conf):\n",
    "    w = tf.ones([conf['batch_size'], hparams['seq_length']])\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits=output_sequence_logits,\n",
    "            targets=tf.cast(Y,tf.int32),\n",
    "            weights=w, average_across_batch= True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_Y(data,seq_len):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    num_of_x = data.shape[0]//seq_len\n",
    "    for i in range(num_of_x):\n",
    "        X.append(data[i*seq_len:(i+1)*seq_len])\n",
    "        Y.append(data[(i*seq_len + 1):((i+1)*seq_len + 1)])\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mini_batches(X,Y,batch_size):\n",
    "    num_of_input = X.shape[0]\n",
    "    num_of_batches = int(num_of_input/batch_size)\n",
    "    mini_batches = []\n",
    "    for i in range(num_of_batches):\n",
    "        start_ind = i*batch_size\n",
    "        batch_x = X[start_ind:start_ind+batch_size]\n",
    "        batch_y = Y[start_ind:start_ind+batch_size]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "    '''if num_of_input%batch_size != 0:\n",
    "        start_ind = num_of_batches*batch_size\n",
    "        batch_x = X[start_ind:]\n",
    "        batch_y = Y[start_ind:]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "    '''\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, hparams, conf):\n",
    "    ops.reset_default_graph()\n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_placeholder(hparams,conf)\n",
    "    parameters = initialize_parameters(hparams,conf)\n",
    "    Z = add_embeddings(X, hparams, conf)\n",
    "    logits = forward_propagation(Z,parameters,hparams, conf)\n",
    "    \n",
    "    output = tf.reshape(tf.concat(values=logits,axis=1), shape=(-1,hparams['seq_length'],conf['vocab_size']))\n",
    "    cost = compute_cost(output,Y,hparams,conf)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams['learning_rate']).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init) #run init\n",
    "        \n",
    "        batch_size = conf['batch_size']\n",
    "        \n",
    "        for epoch in range(hparams['num_epochs']):\n",
    "            minibatch_cost = 0\n",
    "            minibatches = get_mini_batches(X_train,Y_train,batch_size)\n",
    "            num_minibatches = int(X_train.shape[0]/batch_size)\n",
    "            \n",
    "            for ind,minibatch in enumerate(minibatches):\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                _ , temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            costs.append(minibatch_cost)\n",
    "            print(\"training cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "            \n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(X_valid.shape[0]/batch_size)\n",
    "            \n",
    "            for ind,minibatch in enumerate(get_mini_batches(X_valid,Y_valid,batch_size)):\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                temp_cost = sess.run(cost,feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            print(\"validation cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "        \n",
    "        minibatch_cost = 0\n",
    "        num_minibatches = int(X_test.shape[0]/batch_size)\n",
    "        for ind,minibatch in enumerate(get_mini_batches(X_test,Y_test,batch_size)):\n",
    "            (minibatch_X,minibatch_Y) = minibatch\n",
    "            temp_cost = sess.run(cost,feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "        print(\"Test cost after epoch:\" + str(epoch) + \" = \" + str(minibatch_cost))\n",
    "                \n",
    "    return costs,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,Y_train=get_X_Y(train,hparams['seq_length'])\n",
    "X_valid,Y_valid=get_X_Y(valid,hparams['seq_length'])\n",
    "X_test,Y_test=get_X_Y(test,hparams['seq_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10000)\n",
      "training cost after epoch:0 = 6.97579618479\n",
      "validation cost after epoch:0 = 6.57109409571\n",
      "training cost after epoch:1 = 6.49387493828\n",
      "validation cost after epoch:1 = 6.3929575483\n",
      "training cost after epoch:2 = 6.31890151359\n",
      "validation cost after epoch:2 = 6.23338991404\n",
      "training cost after epoch:3 = 6.12448288747\n",
      "validation cost after epoch:3 = 6.04525339603\n",
      "training cost after epoch:4 = 5.9676628097\n",
      "validation cost after epoch:4 = 5.92669697603\n",
      "training cost after epoch:5 = 5.83784895543\n",
      "validation cost after epoch:5 = 5.80317026377\n",
      "training cost after epoch:6 = 5.70150351051\n",
      "validation cost after epoch:6 = 5.69453543425\n",
      "training cost after epoch:7 = 5.58677037031\n",
      "validation cost after epoch:7 = 5.6097706159\n",
      "training cost after epoch:8 = 5.4914667859\n",
      "validation cost after epoch:8 = 5.54200363159\n",
      "training cost after epoch:9 = 5.40889987093\n",
      "validation cost after epoch:9 = 5.48807875315\n",
      "training cost after epoch:10 = 5.3367707003\n",
      "validation cost after epoch:10 = 5.44345510006\n",
      "training cost after epoch:11 = 5.27257573841\n",
      "validation cost after epoch:11 = 5.40565574169\n",
      "training cost after epoch:12 = 5.21426314708\n",
      "validation cost after epoch:12 = 5.37317204475\n",
      "training cost after epoch:13 = 5.16065794585\n",
      "validation cost after epoch:13 = 5.34510093927\n",
      "training cost after epoch:14 = 5.11056763605\n",
      "validation cost after epoch:14 = 5.32067980369\n",
      "training cost after epoch:15 = 5.06383059988\n",
      "validation cost after epoch:15 = 5.29885480801\n",
      "training cost after epoch:16 = 5.01965071981\n",
      "validation cost after epoch:16 = 5.27955573797\n",
      "training cost after epoch:17 = 4.97792272852\n",
      "validation cost after epoch:17 = 5.26363976796\n",
      "training cost after epoch:18 = 4.93825914528\n",
      "validation cost after epoch:18 = 5.24981071552\n",
      "training cost after epoch:19 = 4.90047779462\n",
      "validation cost after epoch:19 = 5.2382559975\n",
      "training cost after epoch:20 = 4.86427250603\n",
      "validation cost after epoch:20 = 5.22874375184\n",
      "training cost after epoch:21 = 4.82936204664\n",
      "validation cost after epoch:21 = 5.22055198749\n",
      "training cost after epoch:22 = 4.79573192581\n",
      "validation cost after epoch:22 = 5.21410286427\n",
      "training cost after epoch:23 = 4.76330601853\n",
      "validation cost after epoch:23 = 5.20853881041\n",
      "training cost after epoch:24 = 4.73150802132\n",
      "validation cost after epoch:24 = 5.20452773571\n",
      "training cost after epoch:25 = 4.70081638974\n",
      "validation cost after epoch:25 = 5.20189446211\n",
      "training cost after epoch:26 = 4.67102619749\n",
      "validation cost after epoch:26 = 5.20062981049\n",
      "training cost after epoch:27 = 4.64201833949\n",
      "validation cost after epoch:27 = 5.20038459698\n",
      "training cost after epoch:28 = 4.61381612156\n",
      "validation cost after epoch:28 = 5.20114348332\n",
      "training cost after epoch:29 = 4.58636452425\n",
      "validation cost after epoch:29 = 5.20311639706\n",
      "training cost after epoch:30 = 4.55956949303\n",
      "validation cost after epoch:30 = 5.20569896698\n",
      "training cost after epoch:31 = 4.53342598637\n",
      "validation cost after epoch:31 = 5.20946514606\n",
      "training cost after epoch:32 = 4.50791996915\n",
      "validation cost after epoch:32 = 5.21340086063\n",
      "training cost after epoch:33 = 4.48299631457\n",
      "validation cost after epoch:33 = 5.21875971556\n",
      "training cost after epoch:34 = 4.4586157254\n",
      "validation cost after epoch:34 = 5.22408459584\n",
      "training cost after epoch:35 = 4.43470359875\n",
      "validation cost after epoch:35 = 5.23059346279\n",
      "training cost after epoch:36 = 4.41132800705\n",
      "validation cost after epoch:36 = 5.23718812068\n",
      "training cost after epoch:37 = 4.38846453294\n",
      "validation cost after epoch:37 = 5.24458332856\n",
      "training cost after epoch:38 = 4.36604052822\n",
      "validation cost after epoch:38 = 5.25197676818\n",
      "training cost after epoch:39 = 4.34410429159\n",
      "validation cost after epoch:39 = 5.26037430763\n",
      "Test cost after epoch:39 = 5.26037430763\n"
     ]
    }
   ],
   "source": [
    "_, parameters = model(X_train, Y_train, X_valid, Y_valid,hparams,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation perplexity:181.34197204\n"
     ]
    }
   ],
   "source": [
    "print('Best Validation perplexity:' + str(np.exp(5.20038459698)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do<br>\n",
    "<ul>\n",
    "<li>Hyperparameter Tuning</li>\n",
    "<li>Add more LSTM layer with dropout in between</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
